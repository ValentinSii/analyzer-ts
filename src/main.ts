import { normalizeArray } from './helpers/normalize';
import { PlaywrightScraper } from "./scraper/PlaywrightScraper";
import { Input, Output } from "../src/types";
import { searchData } from './search/Search';
import { Validator } from './validation/Validator';
import { readFileSync} from "fs";
import { KeyValueStore, log } from '@crawlee/core';
import { Actor } from 'apify';


(async () => {
    if (process.env.NODE_ENV != "production") {
       
        // Copy input from input examples   
        const inputFile = "./src/static/example_inputs/INPUT_MALL.json"
        await KeyValueStore.setValue("INPUT", readFileSync(inputFile), { contentType: "application/json; charset=utf-8" })
        log.debug("Running in dev mode");
    }
})();

/**
 * Actor's entry point. 
 */
(async () => {
    Actor.init();
    log.setLevel(log.LEVELS.DEBUG);

    // Structure of the input is defined in /INPUT_SCHEMA.json.
    // This function expects INPUT.json file in the key-value storage
    const input = await KeyValueStore.getInput() as Input;
    // copy frontend application to keyvalue store, this file is generated by project analyzer-ui, mentioned in the readme.
    // on Apify platform, this file is copied during building in docker
    await KeyValueStore.setValue("DASHBOARD", readFileSync("./src/static/index.html"), { contentType: 'text/html; charset=utf-8' });

    log.info("INPUT", input);
    log.info('===================================================================================================================');
    log.info('Welcome to the page analyzer!');
    log.info('URL: ' + input.url);
    log.info('KEYWORDS: ' + input.keywords);
    log.info('===================================================================================================================');

    const normalizedKeywords = normalizeArray(input.keywords);
    const scraper = new PlaywrightScraper(input.url, normalizedKeywords);

    const output = new Output(input.url, normalizedKeywords);
    output.analysisStarted = new Date().toISOString().replace(/T/, ' ').replace(/\..+/, '');

    // TODO: implement multiple retries 
    try {
        // navigate to the website, scrape and parse the data 
        const scrapedData = await scraper.scrapePage(true, true);

        // after the browser is closed, search the data 
        const searchResults = searchData(scrapedData, normalizedKeywords);

        const validator = new Validator();

        // retrieve initial responses by cheeriocrawler and 
        // validate search results against it
        const validatedData = await validator.validate(input.url, normalizedKeywords, searchResults);

        // save the output
        output.scrapedData = scrapedData;
        output.searchResults = searchResults;
        output.keywordConclusions = validatedData.conclusion;
        output.xhrValidated = validatedData.xhrValidated;

    } catch (e: any) {

        log.error('Top lever error inside main:');
        log.error(e.message);
        console.error(e);
        output.actorSuccess = false;
        output.errorMessage = e.message;

    }
    output.analysisEnded = new Date().toISOString().replace(/T/, ' ').replace(/\..+/, '');
    await KeyValueStore.setValue("OUTPUT", JSON.stringify(output!, null, 2), { contentType: 'application/json; charset=utf-8' });

    Actor.exit();
})();
